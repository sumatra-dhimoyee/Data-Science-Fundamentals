{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRSXpkt_1kEi"
      },
      "source": [
        "# Worksheet 17\n",
        "\n",
        "Name:  \n",
        "UID:\n",
        "\n",
        "### Topics\n",
        "\n",
        "- Recommender Systems\n",
        "\n",
        "### Recommender Systems\n",
        "\n",
        "In the example in class of recommending movies to users we used the movie rating as a measure of similarity between users and movies and thus the predicted rating for a user is a proxy for how highly a movie should be recommended. So the higher the predicted rating for a user, the higher a recommendation it would be.\n",
        "\n",
        "a) Consider a streaming platform that only has \"like\" or \"dislike\" (no 1-5 rating). Describe how you would build a recommender system in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6W7XI6_1kEk"
      },
      "source": [
        "A binary classification task would be used instead of a rating prediction in order to create a recommender system on a streaming platform where users can only select \"like\" or \"dislike\" content. Based on user and movie variables like demographics or genres, you can use models like logistic regression, random forests, or neural networks to predict user preferences. These models are evaluated using measures like accuracy and F1-score, and they are trained using historical like/dislike data. Create a feedback loop to improve predictions based on new user interactions, optimize the model via feature engineering and hyperparameter tuning, and continuously monitor and update the system to accommodate shifting user preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPX-PNi21kEk"
      },
      "source": [
        "b) Describe 3 challenges of building a recommender system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBCehx4V1kEl"
      },
      "source": [
        "Data sparsity: This is the state in which there is insufficient data regarding how users engage with particular things. Consider a brand-new online store with scant customer information. Product recommendations become challenging since the system is unable to accurately learn consumer preferences.\n",
        "\n",
        "Cold Start Issue: This arises while working with brand-new products or people. It is difficult to recommend relevant content to a new user on a streaming platform because they do not have a watch history. Comparably, it is challenging to promote a new product to customers when it is untested and lacking interaction data on the shelf.\n",
        "\n",
        "Balancing Exploration and Exploitation: Recommender systems frequently become stuck proposing content that users already enjoy. This is known as the \"exploration vs. exploitation\" dilemma. This restricts exposure to new things by generating a filter bubble. Finding a balance between recommending well-known favorites (exploitation) and exposing people to novel ideas is the difficult part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9pbl_dx1kEl"
      },
      "source": [
        "c) Why is SVD not an option for collaborative filtering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7DTMNwR1kEl"
      },
      "source": [
        "Because traditional SVD has trouble managing sparse matrices, which have the majority of user-item ratings missing, it is rarely utilized in collaborative filtering. Additionally, this strategy is not easily scalable, and it is difficult to alter suggestions based on fresh data without recomputation. In practical applications, changes such as Funk-SVD are employed to address these problems. These modifications to the SVD technique make it more appropriate for dynamic, large-scale recommender systems by limiting its attention to known ratings, permitting incremental updates, and optimizing its performance with partial data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd4rjcGL1kEm"
      },
      "source": [
        "d) Use the code below to train a recommender system on a dataset of amazon movies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fr6gKLf12x_",
        "outputId": "685ecb48-a1b4-475d-f8aa-31a320a2a0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdBncC3v2DFh",
        "outputId": "53de81c1-528c-4b81-f593-c6f85627dead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e20cb56b1b3c213bbe6f8f8395fe33c86311bdd18f112460b0d2c02cf2e0c316\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0YJkh6a16wq",
        "outputId": "edd1da71-34b0-4d5d-f75e-8f56ed738252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRNDwM_91kEn"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "import pandas as pd\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "findspark.init()\n",
        "conf = SparkConf().set(\"spark.executor.memory\", \"28g\").set(\"spark.driver.memory\", \"28g\").set(\"spark.driver.cores\", \"8\")\n",
        "sc = SparkContext.getOrCreate(conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "init_df = pd.read_csv(\"./train.csv\").dropna()\n",
        "init_df['UserId_fact'] = init_df['UserId'].astype('category').cat.codes\n",
        "init_df['ProductId_fact'] = init_df['ProductId'].astype('category').cat.codes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_processed, X_test_processed, Y_train, Y_test = train_test_split(\n",
        "    init_df.drop(['Score'], axis=1),\n",
        "    init_df['Score'],\n",
        "    test_size=1/4.0,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "X_train_processed['Score'] = Y_train\n"
      ],
      "metadata": {
        "id": "Kwt_VbEh3E_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "df = spark.createDataFrame(X_train_processed[['UserId_fact', 'ProductId_fact', 'Score']])\n",
        "als = ALS(userCol=\"UserId_fact\", itemCol=\"ProductId_fact\", ratingCol=\"Score\", coldStartStrategy=\"drop\", nonnegative=True)\n",
        "\n",
        "param_grid = ParamGridBuilder()\\\n",
        "    .addGrid(als.rank, [50, 100])\\\n",
        "    .addGrid(als.regParam, [0.05, 0.1])\\\n",
        "    .build()\n",
        "\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Score\")\n",
        "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
        "rec_sys = cv.fit(df).bestModel\n"
      ],
      "metadata": {
        "id": "xL-VD1Sf3HMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mean_score = Y_train.mean()\n",
        "rec['prediction'].fillna(mean_score, inplace=True)\n",
        "\n",
        "if len(rec) != len(X_test_processed):\n",
        "    print(\"Warning: Mismatched number of predictions and test instances.\")\n",
        "\n",
        "X_test_processed['predicted_Score'] = rec['prediction']\n",
        "\n",
        "if X_test_processed['predicted_Score'].isna().any() or Y_test.isna().any():\n",
        "    print(\"Error: NaN values found in predictions or actual scores.\")\n",
        "\n",
        "    X_test_processed['predicted_Score'].fillna(mean_score, inplace=True)\n",
        "\n",
        "try:\n",
        "    rmse = mean_squared_error(Y_test, X_test_processed['predicted_Score'], squared=False)\n",
        "    print(\"Kaggle RMSE = \", rmse)\n",
        "except ValueError as e:\n",
        "    print(\"Error during RMSE calculation:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts6xzFfq6dZC",
        "outputId": "691b7c24-453e-4a09-bad5-772f3d7024eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Mismatched number of predictions and test instances.\n",
            "Error: NaN values found in predictions or actual scores.\n",
            "Kaggle RMSE =  1.2638309053449663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sM3zyQqF5EfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}